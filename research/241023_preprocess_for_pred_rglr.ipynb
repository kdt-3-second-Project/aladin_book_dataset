{"cells":[{"cell_type":"markdown","metadata":{"id":"hMv-mcfb1cxL"},"source":["## 요약\n","- XGB를 이용하여 여러가지 상황에서의 성능 확인\n","- hyperparameter 차이에 따른 성능 차이\n","  - grid search 이용해서 찾은 hyperparameter가 default일 때보다 성능이 다소 우위\n","- train set에 SalesPoint를 포함 했을 때와 포함하지 않았을 때\n","  - SalesPoint\n","- val, test set 전체에서의 성능과, train에 포함된 적 없는 도서에 대한 데이터로 제한했을 때의 성능\n","  - hyperparameter가 default일 때의 성능이 다소 우위"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"6Rd3woP61cxV"},"outputs":[],"source":["import os, natsort, re\n","from tqdm import tqdm\n","import time, random"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Jdm14GU9JE1n"},"outputs":[],"source":["from itertools import repeat, chain\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import mean_absolute_percentage_error as mape\n","from sklearn.metrics import mean_squared_error as mse\n","from sklearn.metrics import mean_squared_log_error as msle\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats(\"png2x\")\n","# 테마 설정: \"default\", \"classic\", \"dark_background\", \"fivethirtyeight\", \"seaborn\"\n","mpl.style.use(\"fivethirtyeight\")\n","# 이미지가 레이아웃 안으로 들어오도록 함\n","mpl.rcParams.update({\"figure.constrained_layout.use\": True})\n","mpl.rcParams['axes.unicode_minus'] = False"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":329},"executionInfo":{"elapsed":122677,"status":"error","timestamp":1721614229201,"user":{"displayName":"이스트캠퍼스","userId":"11631653647711445405"},"user_tz":-540},"id":"vRSxgFv_1eJE","outputId":"de14af6d-3a18-47a0-8d18-a2e8faa2b8af"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[1;32m      3\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive/\u001b[39m\u001b[39m'\u001b[39m, force_remount\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive/', force_remount=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"tOybaY2Uo27a"},"outputs":[],"source":["#cd /content/drive/MyDrive/AI3_prjct2_aladin/"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JQwX5d0D2H8a","outputId":"975fee49-7127-4362-c443-3165e8b76dc9"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '/content/drive/MyDrive/WASSUP-ESTsoft-AI/project/project2/'\n","/home/doeun/code/AI/ESTSOFT2024/workspace/2.project_text/aladin_book_price/research\n"]}],"source":["cd /content/drive/MyDrive/WASSUP-ESTsoft-AI/project/project2/"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"fPd7uoaa1cxU"},"outputs":[],"source":["# 로컬에서\n","\n","plt.rc(\"font\", family = \"D2Coding\")\n","plt.rcParams[\"axes.unicode_minus\"] = False"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ZFuEjgvk1cxW"},"outputs":[],"source":["PRJCT_PATH = '/home/doeun/code/AI/ESTSOFT2024/workspace/2.project_text/aladin_usedbook/'\n","#PRJCT_PATH = '/content/drive/MyDrive/WASSUP-ESTsoft-AI/project/project2/'\n","#PRJCT_PATH = '/content/drive/MyDrive/AI3_prjct2_aladin/aladin_usedbook/'\n","save_dir = 'processed/model_input'\n","dir_path = os.path.join(PRJCT_PATH,save_dir)\n","#dir_path = './'"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rI20x4AkwggP","outputId":"2e1482eb-d760-46a3-b32c-97cdf2f2b10b"},"outputs":[{"name":"stdout","output_type":"stream","text":["240710_crawling_step0.ipynb           240718_step0_by_js.ipynb\n","240711_crawling_step1.ipynb           240719_additional_eda.ipynb\n","240711_preprocess_bookinfo.ipynb      240719_simple_model_for_cropped.ipynb\n","240715_encoding_usedinfo.ipynb        240721_GridSearch_for_XGB.ipynb\n","240716_check_bookinfo.ipynb           240721_experiment_w_XGB.ipynb\n","240716_check_bookinfo2.ipynb          240721_hyperparameters_XGB.ipynb\n","240716_encoding_bookinfo.ipynb        241023_basic_model.ipynb\n","240717_simple_model_for_sample.ipynb  241023_preprocess_for_pred_rglr.ipynb\n","240717_split_and_scale.ipynb\n"]}],"source":["ls"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"EKRTD3PdyQsc"},"outputs":[],"source":["import sys\n","sys.path.append(PRJCT_PATH)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-10-24 02:43:06.816031: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-10-24 02:43:07.472055: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-10-24 02:43:07.679526: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2024-10-24 02:43:07.679625: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","2024-10-24 02:43:07.797970: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-10-24 02:43:11.391967: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","2024-10-24 02:43:11.393703: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","2024-10-24 02:43:11.393757: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"]}],"source":["RSLT_DIR = PRJCT_PATH + 'processed/'\n","\n","bookinfo_name = 'bookinfo_ver{}.pkl'.format(0.75)\n","bookinfo_path = os.path.join(RSLT_DIR,bookinfo_name)\n","\n","sys.path.append(PRJCT_PATH)\n","from module_aladin.file_io import load_pkl, save_pkl\n","from module_aladin.data_process import pd_datetime_2_datenum\n","\n","from konlpy.tag import Mecab\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import itertools\n","from sklearn.preprocessing import MinMaxScaler"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def set_corpus_size(freq,size_feat,mode):\n","    # 입력받은 mode와 size_feat에 따라 size 크기 결정\n","    if mode == 'uniform':\n","        cond = freq['counts']>=freq['counts'].iloc[size_feat]\n","        size = np.sum(cond)\n","    elif mode =='ths':\n","        cond = freq[freq['counts'] > size_feat]\n","        size = np.sum(cond)\n","    else :\n","        if size_feat == None : size = len(freq)\n","        elif size_feat > len(data) : size = len(freq)\n","        else : size = size_feat\n","    return size\n","\n","def make_encoding_by_freq(freq,null_val='[PAD]',size_feat=None,mode=None):\n","    #빈도수 기반 정수 인코딩 dict 만들기\n","    # freq : token 별 등장 빈도 (value_count), size_feat : size관련 변수(max_size, ths등), mode : size 결정 방법\n","    df_freq = pd.DataFrame(freq).T\n","    df_freq = df_freq.rename(columns={0:'token',1:'counts'})\n","    temp = df_freq.sort_values(by='counts',ascending=False)\n","    size = set_corpus_size(temp,size_feat,mode)\n","    temp = temp.iloc[:size]\n","    temp['val'] = np.arange(size)+1\n","    temp2 = temp.set_index('token').to_dict()\n","    map_token_encode = temp2['val']\n","    map_token_encode[null_val]=0\n","    return map_token_encode\n","\n","def encode_tokens(map_token,x,oov=True):\n","    oov_val = len(map_token)+1 if oov else 0 \n","    return map_token[x] if x in map_token else oov_val\n","\n","def make_author_encode_map(bookinfo,ths_author):\n","    pvtb = pd.pivot_table(data=bookinfo,index='Author',values='SalesPoint',aggfunc=np.sum)\n","    pvtb = pvtb.sort_values(by='SalesPoint',ascending=False)\n","    author_top_k= pvtb[pvtb['SalesPoint']>=ths_author].index\n","    encode_author = pd.DataFrame({'author' : author_top_k.values,'val':np.arange(1,len(author_top_k)+1)})\n","    encode_author = encode_author.set_index('author')\n","    return encode_author.to_dict()['val']\n","\n","def make_publshr_encode_map(publshr_data,ths_publshr):\n","    stats = publshr_data.value_counts().sort_values(ascending=False)\n","    top_k_val = stats.iloc[ths_publshr]\n","    publshr_top_k = list(stats[stats >= top_k_val].index)\n","    return {\n","        publshr : n+1\n","        for n,publshr in enumerate(publshr_top_k)\n","    }\n","    \n","\n","def make_store_encode_map(store_data):\n","    stores= store_data.value_counts().sort_values(ascending=False)\n","    return {\n","        place : n+1\n","        for n,place in enumerate(stores.index)\n","    }"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["file_name = 'bookinfo_ver{}.csv'.format(1.0)\n","file_path = os.path.join(RSLT_DIR,file_name)\n","bookinfo_raw = pd.read_csv(file_path)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 158084 entries, 0 to 158083\n","Data columns (total 12 columns):\n"," #   Column      Non-Null Count   Dtype \n","---  ------      --------------   ----- \n"," 0   BName       158083 non-null  object\n"," 1   ItemId      158084 non-null  int64 \n"," 2   BName_sub   9007 non-null    object\n"," 3   Author      158084 non-null  object\n"," 4   Publshr     158084 non-null  object\n"," 5   Author_mul  158084 non-null  bool  \n"," 6   Pdate       158084 non-null  int64 \n"," 7   RglPrice    158084 non-null  int64 \n"," 8   SlsPrice    158084 non-null  int64 \n"," 9   SalesPoint  158084 non-null  int64 \n"," 10  Category    158084 non-null  object\n"," 11  Source      158084 non-null  object\n","dtypes: bool(1), int64(5), object(6)\n","memory usage: 13.4+ MB\n"]}],"source":["bookinfo_raw.info()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["cols = bookinfo_raw.columns.to_list()\n","x_idxs, y_idx = [0,2,3,4,5,6,9,10], 7\n","x_cols = [cols[i] for i in x_idxs]\n","y_col = cols[y_idx]\n","\n","data_X, data_y  = bookinfo_raw[x_cols], bookinfo_raw[y_col]"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["Index(['BName', 'BName_sub', 'Author', 'Publshr', 'Author_mul', 'Pdate',\n","       'SalesPoint', 'Category'],\n","      dtype='object')"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(101173, 8)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(101173,)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(25294, 8)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(25294,)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(31617, 8)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(31617,)"]},"metadata":{},"output_type":"display_data"}],"source":["from sklearn.model_selection import train_test_split\n","\n","X_data, X_tst, y_data, y_tst = train_test_split(data_X,data_y,test_size=0.2,random_state=329)\n","X_trn, X_vld, y_trn, y_vld = train_test_split(X_data,y_data,test_size=0.2,random_state=329)\n","\n","display(X_trn.columns)\n","display(X_trn.shape, y_trn.shape)\n","display(X_vld.shape, y_vld.shape)\n","display(X_tst.shape, y_tst.shape)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["data_dict = {\n","    'trn': {\n","        'X': X_trn,\n","        'y': y_trn\n","        },\n","    'vld':{\n","        'X': X_vld,\n","        'y': y_vld\n","        },\n","    'tst':{\n","        'X': X_tst,\n","        'y': y_tst\n","        \n","    }\n","}"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["mecab = Mecab()\n","tokenizer_basic = lambda x : mecab.morphs(x)\n","#apply tokenizer\n","cols_tknz = ['Category','BName','BName_sub']\n","cols_freq = ['Author','Publshr']"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["for mode, data in data_dict.items():\n","    bookinfo = data['X']\n","    for col in cols_tknz:\n","        bookinfo[col] = bookinfo[col].fillna('').apply(tokenizer_basic)\n","    data_dict[mode]['X'] = bookinfo"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 101173 entries, 96986 to 69788\n","Data columns (total 8 columns):\n"," #   Column      Non-Null Count   Dtype \n","---  ------      --------------   ----- \n"," 0   BName       101173 non-null  object\n"," 1   BName_sub   101173 non-null  object\n"," 2   Author      101173 non-null  object\n"," 3   Publshr     101173 non-null  object\n"," 4   Author_mul  101173 non-null  bool  \n"," 5   Pdate       101173 non-null  int64 \n"," 6   SalesPoint  101173 non-null  int64 \n"," 7   Category    101173 non-null  object\n","dtypes: bool(1), int64(2), object(5)\n","memory usage: 6.3+ MB\n"]},{"data":{"text/plain":["None"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 25294 entries, 132734 to 43660\n","Data columns (total 8 columns):\n"," #   Column      Non-Null Count  Dtype \n","---  ------      --------------  ----- \n"," 0   BName       25294 non-null  object\n"," 1   BName_sub   25294 non-null  object\n"," 2   Author      25294 non-null  object\n"," 3   Publshr     25294 non-null  object\n"," 4   Author_mul  25294 non-null  bool  \n"," 5   Pdate       25294 non-null  int64 \n"," 6   SalesPoint  25294 non-null  int64 \n"," 7   Category    25294 non-null  object\n","dtypes: bool(1), int64(2), object(5)\n","memory usage: 1.6+ MB\n"]},{"data":{"text/plain":["None"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 31617 entries, 146028 to 115151\n","Data columns (total 8 columns):\n"," #   Column      Non-Null Count  Dtype \n","---  ------      --------------  ----- \n"," 0   BName       31617 non-null  object\n"," 1   BName_sub   31617 non-null  object\n"," 2   Author      31617 non-null  object\n"," 3   Publshr     31617 non-null  object\n"," 4   Author_mul  31617 non-null  bool  \n"," 5   Pdate       31617 non-null  int64 \n"," 6   SalesPoint  31617 non-null  int64 \n"," 7   Category    31617 non-null  object\n","dtypes: bool(1), int64(2), object(5)\n","memory usage: 2.0+ MB\n"]},{"data":{"text/plain":["None"]},"metadata":{},"output_type":"display_data"}],"source":["display(data_dict['trn']['X'].info())\n","display(data_dict['vld']['X'].info())\n","display(data_dict['tst']['X'].info())"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["#make encoding map\n","bookinfo = data_dict['trn']['X']\n","book_tknzed = bookinfo[cols_tknz].to_dict('series')\n","book_name, book_subname, category = book_tknzed['BName'], book_tknzed['BName_sub'],book_tknzed['Category']\n","tokens = np.array(list(itertools.chain(*book_name.values,*book_subname.values,*category.values)))\n","token_freq = np.unique(tokens,return_counts=True)\n","\n","map_token_encode = make_encoding_by_freq(token_freq,size_feat=32000)\n","encode_1line =lambda x: list(map(lambda y : encode_tokens(map_token_encode,y),x))"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["#아래 ths 는 EDA 결과 제가 자의적으로 정한 내용\n","ths_author = int(np.round(len(book_name)/500)*75)\n","ths_publshr = int(np.round(len(book_name)/500)*5)\n","\n","map_author_encode = make_author_encode_map(bookinfo[['Author','SalesPoint']],ths_author)\n","map_publshr_encode = make_publshr_encode_map(bookinfo['Publshr'],ths_publshr)\n","\n","encode_maps = {\n","    'Author' : lambda x : encode_tokens(map_author_encode,x,oov=False),\n","    'Publshr' : lambda x : encode_tokens(map_publshr_encode,x,oov=False),\n","}"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["maxlens={\n","    'Category' : 5,\n","    'BName' : 30,\n","    'BName_sub' : 25\n","}\n","x_cols = ['BName', 'BName_sub', 'Author',\n","   'Author_mul', 'Publshr', 'Pdate', 'SalesPoint','Category']\n","\n","book_cols = ['BName', 'BName_sub', 'Author', 'Author_mul', 'Publshr', 'Pdate',\n","       'RglPrice', 'SalesPoint', 'Category']\n","xcols_scalar = list(filter(lambda x : x not in cols_tknz,x_cols)) "]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"data":{"text/plain":["['Author_mul', 'SalesPoint']"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["cols_else"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/3 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:00<00:00, 27.77it/s]\n","100%|██████████| 2/2 [00:00<00:00, 106.47it/s]\n","100%|██████████| 2/2 [00:00<00:00, 91.20it/s]\n","100%|██████████| 3/3 [00:05<00:00,  1.76s/it]\n"]}],"source":["#encode X \n","X_encoded=dict()\n","for mode,sample in tqdm(data_dict.items()):\n","    X_mode = sample['X']\n","    #padding and encoding\n","    encoded = pd.DataFrame(index=X_mode.index) \n","    for col in cols_tknz :\n","        padded = pad_sequences(X_mode[col],padding='post',\n","                               maxlen=maxlens[col],\n","                               value='[PAD]',dtype=object)\n","        encoded[col] = list(np.apply_along_axis(encode_1line,0,padded))\n","    #concat encoded\n","    for col in tqdm(cols_freq):\n","        encoded[col] = X_mode[col].map(encode_maps[col])\n","    X_mode['Pdate'] = X_mode['Pdate'].astype(str)\n","#        date_format = '%Y-%m-%d'\n","    date_format = '%Y%m%d'\n","    encoded['Pdate']= pd.to_datetime(X_mode['Pdate'],format=date_format)\n","    encoded['Pdate']= pd_datetime_2_datenum(encoded['Pdate'])\n","    cols_else = list(filter(lambda x : x not in encoded.columns,x_cols))\n","    encoded[cols_else] = X_mode[cols_else]\n","\n","    concat_tknzed =np.apply_along_axis(np.hstack,1,encoded[cols_tknz].values)\n","    x_scalar = encoded[xcols_scalar].values\n","    X = np.hstack((concat_tknzed,x_scalar))\n","    X_encoded[mode] = X    "]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["import copy\n","\n","len_tknzed, len_scalar = sum(maxlens.values()), len(xcols_scalar)\n","scale_partition = [(0,len_tknzed)]+[(len_tknzed+i,len_tknzed+1+i) for i in range(len_scalar)]\n","scaler_list = [copy.deepcopy(MinMaxScaler()) for _ in scale_partition]\n","scalers = list(zip(scale_partition,scaler_list))\n"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"data":{"text/plain":["(101173, 65)"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["X_encoded['trn'].shape"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"data":{"text/plain":["[(0, 60), (60, 61), (61, 62), (62, 63), (63, 64), (64, 65)]"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["scale_partition"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["X_scaled = dict()\n","for mode in ['trn','vld','tst']:\n","    intermid = []\n","    for (cols,scaler) in scalers:\n","        sliced = X_encoded[mode][:,cols[0]:cols[1]]\n","        sliced = sliced.astype(np.float64)\n","        if mode == 'trn': sliced = scaler.fit_transform(sliced)\n","        else : sliced = scaler.transform(sliced)\n","        intermid.append(sliced)\n","\n","    X_scaled[mode] = np.hstack(intermid)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["data_type = 'prd_rglr'\n","strat=0\n","ver=1.0\n","dir_path = os.path.join(RSLT_DIR,'model_input')\n","for mode, x_scaled in X_scaled.items():\n","    save_pkl(dir_path,'{}.v{}_st-{}_X_{}.pkl'.format(data_type,ver,strat,mode),x_scaled)\n","for mode,sample in data_dict.items(): \n","    save_pkl(dir_path,'{}.v{}_st-{}_y_{}.pkl'.format(data_type,ver,strat,mode),sample['y'].to_numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}
